{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders import (\n",
    "    WebBaseLoader, \n",
    "    PyPDFLoader, \n",
    "    Docx2txtLoader,\n",
    ")\n",
    "\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "from typing import List, Optional, Dict, Any\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import pinecone_text\n",
    "from pinecone_text.sparse import BM25Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/brandon/RAG-Chatbot\n",
      "Files in current directory: ['chroma_store', 'requirements.txt', 'ubc-pair-grade-data-master', 'RAG-Chatbot.code-workspace', 'ragbot', '.gitignore', '.env', 'rag_langchain.ipynb', 'error.ipynb']\n",
      "Looking for .env file...\n",
      "âœ… API key found: AIzaSyDcOv...Yug6I\n",
      "API key length: 39\n",
      "\n",
      "ðŸ”„ Initializing ChatGoogleGenerativeAI...\n",
      "âœ… Model initialized successfully!\n",
      "\n",
      "ðŸ”„ Sending test message...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756585553.988645   81354 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1756585553.994128   81354 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Response received:\n",
      "Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Content: Hello back\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Debug: Check if .env file is being loaded\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Files in current directory:\", os.listdir('.'))\n",
    "print(\"Looking for .env file...\")\n",
    "\n",
    "# Get the API key from environment variables\n",
    "api_key_google = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Debug: Check API key\n",
    "if api_key_google:\n",
    "    print(f\"âœ… API key found: {api_key_google[:10]}...{api_key_google[-5:]}\")\n",
    "    print(f\"API key length: {len(api_key_google)}\")\n",
    "else:\n",
    "    print(\"âŒ API key not found!\")\n",
    "    print(\"Available environment variables:\")\n",
    "    for key in os.environ.keys():\n",
    "        if 'GOOGLE' in key.upper() or 'API' in key.upper() or 'KEY' in key.upper():\n",
    "            print(f\"  {key}\")\n",
    "    \n",
    "    # Try alternative names\n",
    "    alt_keys = ['GOOGLE_GENAI_API_KEY', 'GEMINI_API_KEY', 'GOOGLE_AI_API_KEY']\n",
    "    for alt_key in alt_keys:\n",
    "        alt_value = os.getenv(alt_key)\n",
    "        if alt_value:\n",
    "            print(f\"Found alternative key {alt_key}: {alt_value[:10]}...\")\n",
    "            api_key_google = alt_value\n",
    "            break\n",
    "\n",
    "if not api_key_google:\n",
    "    print(\"\\nâŒ No API key found. Please check:\")\n",
    "    print(\"1. Your .env file exists in the current directory\")\n",
    "    print(\"2. Your .env file contains: api_key_google=your_actual_key\")\n",
    "    print(\"3. No spaces around the = sign\")\n",
    "    print(\"4. No quotes around the key unless they're part of the key\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    print(\"\\nðŸ”„ Initializing ChatGoogleGenerativeAI...\")\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-pro\", \n",
    "        google_api_key=api_key_google,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(\"âœ… Model initialized successfully!\")\n",
    "    \n",
    "    print(\"\\nðŸ”„ Sending test message...\")\n",
    "    messages = [HumanMessage(content=\"Hello, world! Please respond with just 'Hello back!'\")]\n",
    "    result = llm.invoke(messages)\n",
    "    \n",
    "    print(\"âœ… Response received:\")\n",
    "    print(f\"Type: {type(result)}\")\n",
    "    print(f\"Content: {result.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error occurred: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    \n",
    "    # Common error troubleshooting\n",
    "    if \"401\" in str(e) or \"Unauthorized\" in str(e):\n",
    "        print(\"\\nðŸ” This looks like an authentication error:\")\n",
    "        print(\"- Check if your API key is correct\")\n",
    "        print(\"- Make sure your Google AI API key is active\")\n",
    "        print(\"- Visit: https://makersuite.google.com/app/apikey\")\n",
    "    \n",
    "    elif \"403\" in str(e) or \"Forbidden\" in str(e):\n",
    "        print(\"\\nðŸ” This looks like a permissions error:\")\n",
    "        print(\"- Your API key might not have access to Gemini Pro\")\n",
    "        print(\"- Check your API quotas and billing\")\n",
    "    \n",
    "    elif \"400\" in str(e) or \"Bad Request\" in str(e):\n",
    "        print(\"\\nðŸ” This looks like a bad request:\")\n",
    "        print(\"- The model name might be incorrect\")\n",
    "        print(\"- Try 'gemini-1.5-pro' or 'gemini-1.5-flash'\")\n",
    "    \n",
    "    elif \"ImportError\" in str(e) or \"ModuleNotFoundError\" in str(e):\n",
    "        print(\"\\nðŸ” This looks like a missing dependency:\")\n",
    "        print(\"- Run: pip install langchain-google-genai google-generativeai\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nðŸ” Unexpected error. Full traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping unsupported file type: .DS_Store\n",
      "Skipping unsupported file type: UBCV_subjects.json\n",
      "Skipping unsupported file type: pair-reports-UBC-instructor-corrections.json\n",
      "Skipping unsupported file type: UBCO_subjects.json\n",
      "Skipping unsupported file type: .DS_Store\n",
      "Skipping unsupported file type: .DS_Store\n",
      "Skipping unsupported file type: .DS_Store\n",
      "Successfully loaded 482448 documents.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    Docx2txtLoader,\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    ")\n",
    "\n",
    "doc_paths = [\"/Users/brandon/Downloads/ubc-pair-grade-data-master\"]\n",
    "docs = []\n",
    "files_to_process = []\n",
    "\n",
    "# First, gather all file paths from the initial `doc_paths`\n",
    "for path_str in doc_paths:\n",
    "    p = Path(path_str)\n",
    "    if not p.exists():\n",
    "        print(f\"Path does not exist: {p}\")\n",
    "        continue\n",
    "    \n",
    "    if p.is_dir():\n",
    "        # If it's a directory, find all files inside it recursively\n",
    "        files_to_process.extend(p.rglob(\"*\"))\n",
    "    elif p.is_file():\n",
    "        # If it's a single file, add it to the list\n",
    "        files_to_process.append(p)\n",
    "\n",
    "# Now, process each collected file path\n",
    "for file_path in files_to_process:\n",
    "    if not file_path.is_file():\n",
    "        continue  # Skip any directories that might have been included\n",
    "\n",
    "    try:\n",
    "        loader = None\n",
    "        # Check the file extension and select the appropriate loader\n",
    "        if file_path.suffix == \".csv\":\n",
    "            loader = CSVLoader(file_path=str(file_path))\n",
    "        elif file_path.suffix == \".pdf\":\n",
    "            loader = PyPDFLoader(str(file_path))\n",
    "        elif file_path.suffix == \".docx\":\n",
    "            loader = Docx2txtLoader(str(file_path))\n",
    "        elif file_path.suffix in [\".txt\", \".md\"]:\n",
    "            loader = TextLoader(str(file_path))\n",
    "        else:\n",
    "            # You can uncomment the line below to see which files are being skipped.\n",
    "            print(f\"Skipping unsupported file type: {file_path.name}\")\n",
    "            continue\n",
    "\n",
    "        # Load the document and add it to our list\n",
    "        if loader:\n",
    "            docs.extend(loader.load())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document {file_path.name}: {e}\")\n",
    "\n",
    "# You can check how many documents were loaded\n",
    "print(f\"Successfully loaded {len(docs)} documents.\")\n",
    "\n",
    "\n",
    "url = \"https://docs.streamlit.io/develop/quick-reference/release-notes\"\n",
    "try:\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading document from {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split docs\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "document_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brandon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BM25Encoder.__init__() got an unexpected keyword argument 'stopwords'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mpinecone_text\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Encoder\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m bm25 = \u001b[43mBM25Encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43menglish\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m bm25.fit([d.page_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m document_chunks])\n\u001b[32m     11\u001b[39m  \u001b[38;5;66;03m# Stopwords are the common words in a language that usually carry little meaning by themselves.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# BM25 (and other sparse retrieval methods like TF-IDF) rank documents by token frequency.#\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# If you donâ€™t remove stopwords, words like â€œtheâ€ or â€œisâ€ dominate counts but add no signal.\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: BM25Encoder.__init__() got an unexpected keyword argument 'stopwords'"
     ]
    }
   ],
   "source": [
    "import os, nltk\n",
    "os.environ[\"NLTK_DATA\"] = os.path.expanduser(\"~/nltk_data\")\n",
    "nltk.data.path.append(os.path.expanduser(\"~/nltk_data\"))\n",
    "nltk.download(\"stopwords\")  # should print True\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "bm25 = BM25Encoder(stopwords=set(stopwords.words(\"english\")))\n",
    "bm25.fit([d.page_content for d in document_chunks])\n",
    " # Stopwords are the common words in a language that usually carry little meaning by themselves.\n",
    "# BM25 (and other sparse retrieval methods like TF-IDF) rank documents by token frequency.#\n",
    "# If you donâ€™t remove stopwords, words like â€œtheâ€ or â€œisâ€ dominate counts but add no signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing first 48249/482499 chunks (fraction=0.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756585633.496404   81354 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF built: 48249 docs, vocab ~13374\n",
      "Embedded 8000/48249 (batch=64)\n",
      "Embedded 16000/48249 (batch=64)\n",
      "Embedded 24000/48249 (batch=64)\n",
      "Embedded 32000/48249 (batch=64)\n",
      "Embedded 40000/48249 (batch=64)\n",
      "Embedded 48000/48249 (batch=64)\n",
      "Embedded 48249/48249 (batch=64)\n",
      "âœ… Upserted 48249 hybrid vectors to 'ubc-grades-hybrid' (ns='ubc-grades').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Campus: UBCV\\nYear: 2023\\nSession: W\\nSubject: CPSC\\nCourse: 221\\nDetail: \\nSection: 101\\nTitle: Basic Algorithms and Data Stru',\n",
       " 'Campus: UBCV\\nYear: 2023\\nSession: W\\nSubject: CPSC\\nCourse: 221\\nDetail: \\nSection: 203\\nTitle: Basic Algorithms and Data Stru',\n",
       " 'Campus: UBCV\\nYear: 2024\\nSession: W\\nSubject: CPSC\\nCourse: 221\\nDetail: \\nSection: 201\\nTitle: Basic Algorithms and Data Stru',\n",
       " 'Campus: UBCV\\nYear: 2024\\nSession: W\\nSubject: CPSC\\nCourse: 221\\nDetail: \\nSection: 101\\nTitle: Basic Algorithms and Data Stru',\n",
       " 'Campus: UBCV\\nYear: 2023\\nSession: W\\nSubject: CPSC\\nCourse: 221\\nDetail: \\nSection: 201\\nTitle: Basic Algorithms and Data Stru']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Hybrid indexing (first 10% in order) with batching + retries (DOTPRODUCT index) ----\n",
    "import os, re, math, time, random\n",
    "from typing import List, Iterable\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# ======= Config =======\n",
    "FRACTION = 0.10            # first 10% of chunks; set 1.0 for full\n",
    "EMBED_INIT_BATCH = 64      # start batch for Gemini\n",
    "EMBED_MIN_BATCH  = 8       # min batch if we see 504/429/etc.\n",
    "EMBED_MAX_RETRIES = 6\n",
    "UPSERT_BATCH = 100\n",
    "NAMESPACE = \"ubc-grades\"\n",
    "\n",
    "# ======= Preconditions =======\n",
    "assert os.getenv(\"GEMINI_API_KEY\"), \"Missing GEMINI_API_KEY\"\n",
    "assert os.getenv(\"PINECONE_API_KEY\"), \"Missing PINECONE_API_KEY\"\n",
    "assert \"document_chunks\" in globals() and len(document_chunks) > 0, \"document_chunks empty\"\n",
    "\n",
    "# ---- Dense (Gemini) ----\n",
    "emb = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    google_api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "def l2_normalize(vec: List[float]) -> List[float]:\n",
    "    n = math.sqrt(sum(v*v for v in vec)) or 1.0\n",
    "    return [v / n for v in vec]\n",
    "\n",
    "def adaptive_embed_documents(\n",
    "    emb: GoogleGenerativeAIEmbeddings,\n",
    "    texts: List[str],\n",
    "    init_batch: int = EMBED_INIT_BATCH,\n",
    "    min_batch: int = EMBED_MIN_BATCH,\n",
    "    max_retries: int = EMBED_MAX_RETRIES,\n",
    "    base_sleep: float = 1.0,\n",
    "    jitter: float = 0.25,\n",
    "    progress_every: int = 1000,\n",
    "    normalize: bool = True,\n",
    ") -> List[List[float]]:\n",
    "    out: List[List[float]] = []\n",
    "    i, B, N = 0, init_batch, len(texts)\n",
    "    while i < N:\n",
    "        batch = texts[i : min(i+B, N)]\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                vecs = emb.embed_documents(batch)\n",
    "                if normalize:\n",
    "                    vecs = [l2_normalize(v) for v in vecs]\n",
    "                out.extend(vecs)\n",
    "                i += len(batch)\n",
    "                if i % progress_every == 0 or i == N:\n",
    "                    print(f\"Embedded {i}/{N} (batch={B})\")\n",
    "                if B < init_batch:  # gentle ramp back up\n",
    "                    B = min(init_batch, B*2)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                err = str(e).lower()\n",
    "                transient = any(t in err for t in [\"504\", \"deadline\", \"timeout\", \"503\", \"500\", \"429\"])\n",
    "                attempt += 1\n",
    "                if transient and B > min_batch:\n",
    "                    B = max(min_batch, B // 2)\n",
    "                    print(f\"[warn] {e} -> reduce batch to {B} and retryâ€¦\")\n",
    "                    time.sleep(base_sleep + random.random()*jitter)\n",
    "                    continue\n",
    "                if transient and attempt <= max_retries:\n",
    "                    sleep = base_sleep*(2**(attempt-1)) + random.random()*jitter\n",
    "                    print(f\"[warn] {e} -> retry {attempt}/{max_retries} in {sleep:.1f}sâ€¦\")\n",
    "                    time.sleep(sleep)\n",
    "                    continue\n",
    "                raise\n",
    "    return out\n",
    "\n",
    "# ---- Take the first 10% in order ----\n",
    "N_total = len(document_chunks)\n",
    "k = max(1, int(N_total * FRACTION))\n",
    "indices = list(range(k))  # first k chunk indices\n",
    "print(f\"Indexing first {k}/{N_total} chunks (fraction={FRACTION:.2f})\")\n",
    "\n",
    "sampled_chunks = [document_chunks[i] for i in indices]\n",
    "corpus_texts = [d.page_content for d in sampled_chunks]\n",
    "\n",
    "# ---- Sparse (TF-IDF; no NLTK) ----\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "token_pattern = r\"[A-Za-z0-9]+\"\n",
    "vectorizer = TfidfVectorizer(\n",
    "    token_pattern=token_pattern,\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    min_df=2\n",
    ")\n",
    "X = vectorizer.fit_transform(corpus_texts)  # CSR matrix\n",
    "print(f\"TF-IDF built: {X.shape[0]} docs, vocab ~{X.shape[1]}\")\n",
    "\n",
    "# ---- Pinecone index (create if missing) â€” MUST be dotproduct for hybrid ----\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "base_name = os.getenv(\"PINECONE_INDEX_NAME\", \"ubc-grades\")\n",
    "index_name = f\"{base_name}-hybrid\"   # new index so we don't clash with old cosine index\n",
    "dim = len(emb.embed_query(\"dimension check\"))  # auto-detect\n",
    "\n",
    "if index_name not in [x[\"name\"] for x in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dim,\n",
    "        metric=\"dotproduct\",  # <-- REQUIRED for dense+sparse hybrid\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=os.getenv(\"PINECONE_CLOUD\", \"aws\"),\n",
    "            region=os.getenv(\"PINECONE_REGION\", \"us-east-1\"),\n",
    "        ),\n",
    "    )\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# ---- Dense embeddings (batched + retries) ----\n",
    "dense_vecs = adaptive_embed_documents(emb, corpus_texts)\n",
    "\n",
    "# ---- Upsert hybrid (dense + sparse) ----\n",
    "metas = [d.metadata | {\"text\": d.page_content} for d in sampled_chunks]\n",
    "\n",
    "buf = []\n",
    "for j, i_doc in enumerate(indices):\n",
    "    row = X.getrow(j)\n",
    "    idxs = row.indices.tolist()\n",
    "    vals = row.data.astype(\"float32\").tolist()\n",
    "\n",
    "    buf.append({\n",
    "        \"id\": f\"{NAMESPACE}-{i_doc}\",                 # keep original global index in the id\n",
    "        \"values\": dense_vecs[j],                      # dense Gemini\n",
    "        \"sparse_values\": {\"indices\": idxs, \"values\": vals},  # sparse TF-IDF\n",
    "        \"metadata\": metas[j],\n",
    "    })\n",
    "    if len(buf) == UPSERT_BATCH:\n",
    "        index.upsert(vectors=buf, namespace=NAMESPACE)\n",
    "        buf = []\n",
    "if buf:\n",
    "    index.upsert(vectors=buf, namespace=NAMESPACE)\n",
    "\n",
    "print(f\"âœ… Upserted {len(indices)} hybrid vectors to '{index_name}' (ns='{NAMESPACE}').\")\n",
    "\n",
    "# --- Hybrid query (dense + sparse with alpha) ---\n",
    "def hybrid_query(index, vectorizer, emb, query: str, top_k=10, alpha=0.6, flt=None, namespace=\"ubc-grades\"):\n",
    "    # dense\n",
    "    q_dense = l2_normalize(emb.embed_query(query))\n",
    "    q_dense = [alpha * x for x in q_dense]\n",
    "    # sparse\n",
    "    q_row = vectorizer.transform([query])\n",
    "    q_idx = q_row.indices.tolist()\n",
    "    q_val = [(1 - alpha) * float(v) for v in q_row.data.tolist()]\n",
    "    # search\n",
    "    return index.query(\n",
    "        vector=q_dense,\n",
    "        sparse_vector={\"indices\": q_idx, \"values\": q_val},\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        filter=flt,\n",
    "        namespace=namespace\n",
    "    )\n",
    "\n",
    "# quick smoke test:\n",
    "res = hybrid_query(index, vectorizer, emb, \"cpsc 221 2023w average\", top_k=5, alpha=0.6)\n",
    "[m[\"metadata\"].get(\"text\",\"\")[:120] for m in res.get(\"matches\",[])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "class PineconeHybridRetriever:\n",
    "    def __init__(self, index, vectorizer, emb, namespace=\"ubc-grades\", alpha=0.6, top_k=8, flt=None):\n",
    "        self.index, self.vectorizer, self.emb = index, vectorizer, emb\n",
    "        self.namespace, self.alpha, self.top_k, self.flt = namespace, alpha, top_k, flt\n",
    "\n",
    "    def invoke(self, query: str):\n",
    "        # dense part\n",
    "        import math\n",
    "        def l2(v): \n",
    "            n = math.sqrt(sum(x*x for x in v)) or 1.0\n",
    "            return [x/n for x in v]\n",
    "        q_dense = [self.alpha * x for x in l2(self.emb.embed_query(query))]\n",
    "        # sparse part\n",
    "        q_row = self.vectorizer.transform([query])\n",
    "        q_idx = q_row.indices.tolist()\n",
    "        q_val = [(1 - self.alpha) * float(v) for v in q_row.data.tolist()]\n",
    "        res = self.index.query(\n",
    "            vector=q_dense,\n",
    "            sparse_vector={\"indices\": q_idx, \"values\": q_val},\n",
    "            top_k=self.top_k,\n",
    "            include_metadata=True,\n",
    "            filter=self.flt,\n",
    "            namespace=self.namespace,\n",
    "        )\n",
    "        docs = []\n",
    "        for m in res.get(\"matches\", []):\n",
    "            md = m.get(\"metadata\", {}) or {}\n",
    "            txt = md.pop(\"text\", \"\")\n",
    "            docs.append(Document(page_content=txt, metadata=md | {\"_score\": m.get(\"score\")}))\n",
    "        return docs\n",
    "\n",
    "retriever = PineconeHybridRetriever(index, vectorizer, emb, alpha=0.6, top_k=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756590913.570250   81354 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1756590913.592141   81354 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average for CPSC 221 in 2023W varied depending on the section:\n",
      "\n",
      "* **Section 101:** 80.9 (CPSC 221, 2023W)\n",
      "* **Section 201:** 79.0 (CPSC 221, 2023W)\n",
      "* **Section 202:** 78.5 (CPSC 221, 2023W)\n",
      "* **Section 203:** 81.1 (CPSC 221, 2023W)\n",
      "\n",
      "There is no single average for the entire course in 2023W.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"models/gemini-1.5-flash\",  # or pro\n",
    "    google_api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful assistant for UBC grades. Use the provided context. \"\n",
    "     \"If the course/term/instructor is missing, ask for it. If unknown, say so.\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer with citations (course/term).\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    out = []\n",
    "    for d in docs[:10]:\n",
    "        c = d.metadata.get(\"course\", \"\")\n",
    "        t = d.metadata.get(\"term\", d.metadata.get(\"year\", \"\"))\n",
    "        out.append(f\"- [{c} {t}] {d.page_content[:550]}\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def answer(question: str):\n",
    "    ctx_docs = retriever.invoke(question)\n",
    "    return llm.invoke(PROMPT.format_messages(question=question, context=format_docs(ctx_docs))).content\n",
    "\n",
    "print(answer(\"What was the average for CPSC 221 in 2023W?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
